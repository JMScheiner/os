/**

@mainpage 15-410 Project 3
@author Tim Wilson (tjwilson) 
@author Justin Scheiner (jscheine)

Decisions we made (in no particular order)
- We have a global pcb/tcb that we initialize on startup and jump to
  whenerver no one else can usefully run. This is the equivalent of the
  idle program.

- Debug statements are all encapsulated in debug_print statements that
  work like lprintf statements, but take an additional string. The debug
  statement only prints if the additional argument is in a list of strings
  currently being debugged. This is useful for debugging a specific
  complex component like readline without seeing log messages for the rest
  of the kernel.

- To vanish as the last thread in a process, the thread first sets a flag
  to indicate that it is vanishing.  Next the thread notifies its children
  it is dying and frees any uncollected statuses. The thread's parent is
  then notified and told not to exit until we remove ourselves from their
  child list and deliver our status. After this, we wait for all of our
  children who are currently vanishing to vanish before finally vanishing
  ourselves.
  In all cases, a thread frees the kernel stack of the previous thread
  that exited and removes itself from the global thread table.

- To wait, a thread first acquires a check lock to ensure that there are
  enough child threads available to wait on. If this passes, the thread
  waits until a status has been made available to it by a child thread.
  Finally, we collect this status (which is allocated seperately from the
  child tcb) and free it.

<b> Memory Organization: </b>

Memory is arranged into three major segments:
   
- Direct Mapped kernel memory <code>[0x0, USER_MEM_START]</code>
   -# Kernel heap

- User Memory <code>[USER_MEM_START, USER_MEM_END] </code>
   
- Kernel Virtual Memory <code>[kvm_bottom, KERNEL_MEM_END]</code>
   -# Page Tables
   -# Page Directories
   -# Virtual Page Directories
   -# Kernel Stacks

Page tables for kernel memory are global and shared by all threads - they 
 exist in the kernel heap and are allocated with smemalign. This was a design 
 decision to simplify the interaction of different processes, and cause fewer 
 complications during context switching and vanishing. 

Free frames are managed in a simple linked list structure, and zero'd on allocation. 
The link structure is contained in the frames themselves, and is protected by the <code>user_free_lock</code>.
There is a globally mapped <code>FREE_PAGE</code> whose access is also protected by the <code>user_free_lock</code>, 
which allows us to write the link structure into the page. 
Zeroing occurs on allocation 
since we always map the page in our own address space on allocation, whereas this isn't 
necessarily true on freeing.  This dance occurs in mm_new_frame and mm_free_frame. 

<b>Kernel Virtual Memory (KVM):</b> manages addressable space with a variable <code>kvm_bottom</code> which descends as 
   demands for kernel pages arise. Freed kernel pages enter a <code>kernel_free_list</code> which is separate
   from the <code>user_free_list</code>. On KVM allocation, we first attempt to allocate
   free kernel pages, and then dip into the user free frame pool if necessary (e.g. every page 
   between <code>kvm_bottom</code> and <code>KERNEL_MEM_END</code> is in use). Every 4M of kernel 
   memory we allocate a new KVM table. This is an expensive operation that blocks subsequent 
   KVM allocations, since we need to take time to map the new table in every existing
   processes directory. Fortunately, it doesn't happen often, and generally happens less over time,
   as frames dedicated to the user shift to the kernel as necessary.

<b> Request/Allocate </b> To simplify error handling (e.g. out of frames) allocation is separated into requests and 
 allocations. Requests are only granted if there will be enough frames to meet the demand, after the request is 
 granted frame allocations are unchecked, because they are guaranteed to pass. Allocations only occur in
 <code> mm_duplicate_address_space </code> and <code> mm_alloc</code>
 
<b> Address Space Copying </b> Address space copying seeks through the current processes user 
memory to determine the number of kernel / user frames we will need, and to find a free addressable
page (in a table that already exists) to perform the copy "through." If no such page exists, a 
"scratch" table is allocated between <code>USER_MEM_END</code> and <code>KERNEL_MEM_START</code>.
It then builds the new address space by duplicating the current one. 

<b> Region Lists </b> User space is managed in a region list, which is responsible for slicing 
 up user memory in an intelligent way, and deploying the right page fault handler during page 
 faults. For example, we can use the region manager to determine what to do on <code>new_pages</code> / <code>remove_pages</code>
 calls. 

<b> Heap Occupants: </b> The heap is occupied by PCBs, statuses nodes, 
   the tid->tcb heap, the sleep heap, global page tables, and region nodes. 
   That is all. 

<b> Virtual Directories: </b> Since the user page tables are not direct mapped, we are required to keep 
   a directory of table virtual addresses along with the physical directory
   used by the memory managmenent hardware. An offset into the virtual 
   directory gives the page aligned (virtual) address of the user table 
   (in the range <code>[0xf0000000, 0xffffffff]</code>)

*/
